{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5627a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85a15efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0cc422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"AIzaSyD4jxQUrXRoIOU5cZ4oOnprxDaYl-VUuuM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6d23c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GooglePalm Model\n",
    "\n",
    "llm = GooglePalm(google_api_key = api_key, temperature = 0.3)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427b4cc",
   "metadata": {},
   "source": [
    "- Temperature\n",
    "    - means how creative your model need to be\n",
    "    - If temp is set to 0, it means it is safe and not taking any risk\n",
    "    - But, if it is 0.1 or 1 it might take risk and might generate wrong o/p but it will be very creative at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "152246ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurant **Il Forno**\n"
     ]
    }
   ],
   "source": [
    "# Lets say i want a fancy restaurant name for italian food\n",
    "\n",
    "name = llm(\"i want to open a restaurant for italian food. Suggest a fancy name for this\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a115c466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i want to open restaurant for mexican food. Suggest a fancy name for this.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just passing a simple text, Here i don't want to keep on changing the ciusine text for this we can use \"PromptTemplate\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"i want to open restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
    ") \n",
    "\n",
    "prompt_template_name.format(cuisine = \"mexican\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b96cf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i want to open restaurant for italia food. Suggest a fancy name for this.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_name.format(cuisine = \"italia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed9a37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KRISHNA\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**The Old Fashioned American Diner**'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "name_chain = LLMChain(llm = llm, prompt = prompt_template_name)\n",
    "name_chain.run(\"american\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3881f0",
   "metadata": {},
   "source": [
    "- chains is one of the most important object in langchain\n",
    "- in chain we pass llm and promp template \n",
    "- as you can see in chain.run() we don't need to pass the all the sentence we just nee to pass the cuisine name the variable\n",
    "- So internally it is calling OpenAI API which we made that connection using via this \"import GooglePalm\"model and api_key\n",
    "- if you are using hugging face you can use your hugging face setup or OIpenAI,Google palm,Lama,Lama2 and many more.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f632ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets say we want to generate a food menu item for the selected cuisine\n",
    "# we will be trying SimpleSequentialChain\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"i want to open restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
    ") \n",
    "name_chain = LLMChain(llm = llm, prompt = prompt_template_name)\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"\"\"Suggest some menu items for {restaurant_name}. Return it as comma separated list\"\"\"\n",
    ") \n",
    "food_items_chain = LLMChain(llm = llm, prompt = prompt_template_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82d45f1",
   "metadata": {},
   "source": [
    "- i above this we just did a simple passing way like.. the output of the 1st step is the input of the second step\n",
    "- we will pass the input cuisine and the output of the the first step is \"resturant name\", will pass through the second template as input variable \n",
    "- hitting the template it will generate the food items\n",
    "- we will be doing this process using \"SimpleSequentialChain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b783c81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tikka Masala, Naan, Curry, Biryani, Dosa, Vindaloo, Samosa\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
    "response = chain.run(\"indian\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e50ac2b",
   "metadata": {},
   "source": [
    "- In simple sequential chain it give one output \n",
    "- Need both output, for that we use \"sequential chain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "200f5a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template for SequentialChain\n",
    "# A SequentialChain can have multiple i/p and o/p\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = \"i want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
    ") \n",
    "name_chain = LLMChain(llm = llm, prompt = prompt_template_name, output_key=\"restaurant_name\")\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = \"Suggest some menu items for {restaurant_name}.\"\n",
    ") \n",
    "food_items_chain = LLMChain(llm = llm, prompt = prompt_template_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4136d777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KRISHNA\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'indian', 'restaurant_name': '* The Tandoori Palace\\n* The Bombay Bistro\\n* The Curry Club\\n* The Masala Hut\\n* The Naan Stop', 'menu_items': '**The Tandoori Palace**\\n\\n* Tandoori chicken\\n* Chicken tikka masala\\n* Lamb biryani\\n* Vegetable korma\\n* Naan bread\\n* Mango lassi\\n\\n**The Bombay Bistro**\\n\\n* Butter chicken\\n* Chicken vindaloo\\n* Lamb rogan josh\\n* Paneer tikka masala\\n* Saag paneer\\n* Gulab jamun\\n\\n**The Curry Club**\\n\\n* Chicken jalfrezi\\n* Beef madras\\n* Lamb pasanda\\n* Vegetable biryani\\n* Naan bread\\n* Chai tea\\n\\n**The Masala Hut**\\n\\n* Chicken tikka masala\\n* Lamb korma\\n* Vegetable biryani\\n* Naan bread\\n* Mango lassi\\n* Kulfi\\n\\n**The Naan Stop**\\n\\n* Naan bread\\n* Tandoori chicken\\n* Chicken tikka masala\\n* Lamb biryani\\n* Vegetable korma\\n* Mango lassi'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain1 = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', 'menu_items']\n",
    ")\n",
    "\n",
    "resp = chain1({'cuisine': 'indian'})\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49cbc6b",
   "metadata": {},
   "source": [
    "- SimpleSequentialChain and SequentialChain both have small difference but the changese are high\n",
    "- in SequentialChain in each LLMChain we will be addingf output_key this out put keys along with the input variable will make the the difference in result by passing it to SequentialChain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e95d79",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "517a1e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The Liberty Bell**\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm = llm, prompt = prompt_template_name)\n",
    "name = chain.run(\"american\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76787d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tikka Masala Palace\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"india\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b27f4fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Config',\n",
       " 'InputType',\n",
       " 'OutputType',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__config__',\n",
       " '__custom_root_type__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__exclude_fields__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_validators__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__include_fields__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__json_encoder__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__post_root_validators__',\n",
       " '__pre_root_validators__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__ror__',\n",
       " '__schema_cache__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__try_update_forward_refs__',\n",
       " '__validators__',\n",
       " '__weakref__',\n",
       " '_abatch_with_config',\n",
       " '_abc_impl',\n",
       " '_acall',\n",
       " '_acall_with_config',\n",
       " '_atransform_stream_with_config',\n",
       " '_batch_with_config',\n",
       " '_calculate_keys',\n",
       " '_call',\n",
       " '_call_with_config',\n",
       " '_chain_type',\n",
       " '_copy_and_set_values',\n",
       " '_decompose_class',\n",
       " '_enforce_dict_if_root',\n",
       " '_get_num_tokens',\n",
       " '_get_value',\n",
       " '_init_private_attributes',\n",
       " '_is_protocol',\n",
       " '_iter',\n",
       " '_lc_kwargs',\n",
       " '_parse_generation',\n",
       " '_run_output_key',\n",
       " '_transform_stream_with_config',\n",
       " '_validate_inputs',\n",
       " '_validate_outputs',\n",
       " 'aapply',\n",
       " 'aapply_and_parse',\n",
       " 'abatch',\n",
       " 'abatch_as_completed',\n",
       " 'acall',\n",
       " 'agenerate',\n",
       " 'ainvoke',\n",
       " 'apply',\n",
       " 'apply_and_parse',\n",
       " 'apredict',\n",
       " 'apredict_and_parse',\n",
       " 'aprep_prompts',\n",
       " 'arun',\n",
       " 'assign',\n",
       " 'astream',\n",
       " 'astream_events',\n",
       " 'astream_log',\n",
       " 'atransform',\n",
       " 'batch',\n",
       " 'batch_as_completed',\n",
       " 'bind',\n",
       " 'callback_manager',\n",
       " 'callbacks',\n",
       " 'config_schema',\n",
       " 'config_specs',\n",
       " 'configurable_alternatives',\n",
       " 'configurable_fields',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'create_outputs',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'from_string',\n",
       " 'generate',\n",
       " 'get_graph',\n",
       " 'get_input_schema',\n",
       " 'get_lc_namespace',\n",
       " 'get_name',\n",
       " 'get_output_schema',\n",
       " 'get_prompts',\n",
       " 'input_keys',\n",
       " 'input_schema',\n",
       " 'invoke',\n",
       " 'is_lc_serializable',\n",
       " 'json',\n",
       " 'lc_attributes',\n",
       " 'lc_id',\n",
       " 'lc_secrets',\n",
       " 'llm',\n",
       " 'llm_kwargs',\n",
       " 'map',\n",
       " 'memory',\n",
       " 'metadata',\n",
       " 'name',\n",
       " 'output_key',\n",
       " 'output_keys',\n",
       " 'output_parser',\n",
       " 'output_schema',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'pick',\n",
       " 'pipe',\n",
       " 'predict',\n",
       " 'predict_and_parse',\n",
       " 'prep_inputs',\n",
       " 'prep_outputs',\n",
       " 'prep_prompts',\n",
       " 'prompt',\n",
       " 'raise_callback_manager_deprecation',\n",
       " 'return_final_only',\n",
       " 'run',\n",
       " 'save',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'set_verbose',\n",
       " 'stream',\n",
       " 'tags',\n",
       " 'to_json',\n",
       " 'to_json_not_implemented',\n",
       " 'transform',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'verbose',\n",
       " 'with_config',\n",
       " 'with_fallbacks',\n",
       " 'with_listeners',\n",
       " 'with_retry',\n",
       " 'with_types']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2c291bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6ceabc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The Old Fashioned American Diner**\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm = llm, prompt = prompt_template_name, memory=memory)\n",
    "name = chain.run(\"american\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c8ac2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* The Tandoori Palace\n",
      "* The Bombay Bistro\n",
      "* The Curry House\n",
      "* The Masala Cafe\n",
      "* The Naan Stop\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"indian\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22b1ed35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LLMChain(memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='american'), AIMessage(content='**The Old Fashioned American Diner**'), HumanMessage(content='indian'), AIMessage(content='* The Tandoori Palace\\n* The Bombay Bistro\\n* The Curry House\\n* The Masala Cafe\\n* The Naan Stop')])), prompt=PromptTemplate(input_variables=['cuisine'], template='i want to open a restaurant for {cuisine} food. Suggest a fancy name for this.'), llm=GooglePalm(client=<module 'google.generativeai' from 'C:\\\\Users\\\\KRISHNA\\\\anaconda3\\\\lib\\\\site-packages\\\\google\\\\generativeai\\\\__init__.py'>, google_api_key=SecretStr('**********'), temperature=0.3)),\n",
       " ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='american'), AIMessage(content='**The Old Fashioned American Diner**'), HumanMessage(content='indian'), AIMessage(content='* The Tandoori Palace\\n* The Bombay Bistro\\n* The Curry House\\n* The Masala Cafe\\n* The Naan Stop')])))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain,memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b9df80f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: american\n",
      "AI: **The Old Fashioned American Diner**\n",
      "Human: indian\n",
      "AI: * The Tandoori Palace\n",
      "* The Bombay Bistro\n",
      "* The Curry House\n",
      "* The Masala Cafe\n",
      "* The Naan Stop\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458db6c",
   "metadata": {},
   "source": [
    "- this is good we can use to store these database as saved transcript of customer conversation\n",
    "- the problem with this ConversationBufferMemory it will keep on growing endlessly and open AI chargers per token, your cost is gonna growup\n",
    "- if you want to lower the cost you just have to restrict the buffer size, like remember the last 10 or 5 conversational exchange\n",
    "- and this thing can be done by ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0bbdc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'input'], template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=llm)\n",
    "convo.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bfb2d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "067ca09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'West Indies'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"who won first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66660f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"what is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66a6451c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clive Lloyd'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"who was captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4c29c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='who won first cricket world cup?'), AIMessage(content='West Indies'), HumanMessage(content='what is 5+5?'), AIMessage(content='10'), HumanMessage(content='who was captain of the winning team?'), AIMessage(content='Clive Lloyd')]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646155cf",
   "metadata": {},
   "source": [
    "- ConversationChain comes with inbuilt ConversationBufferMemory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b64dae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: who won first cricket world cup?\n",
      "AI: West Indies\n",
      "Human: what is 5+5?\n",
      "AI: 10\n",
      "Human: who was captain of the winning team?\n",
      "AI: Clive Lloyd\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740491b2",
   "metadata": {},
   "source": [
    "- When you make a next call like that it will send the entier coversation, it will cost \n",
    "- To tacle that we can say send only last 10 or 20 of the conversational exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8d41f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import  ConversationBufferWindowMemory\n",
    "\n",
    "memory1 = ConversationBufferWindowMemory(k=1)        # last 1 conversational exchange \"k=1\"\n",
    "\n",
    "convo1 = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory  = memory1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "429561b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'West Indies'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "convo1.run(\"who won first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ef2e7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo1.run(\"what is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "934d5cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lionel Messi'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo1.run(\"who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3997bed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: who was the captain of the winning team?\n",
      "AI: Lionel Messi\n"
     ]
    }
   ],
   "source": [
    "print(convo1.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d681c",
   "metadata": {},
   "source": [
    "- you can see it gets forget after one conversational exchange it dosn't remember it "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
