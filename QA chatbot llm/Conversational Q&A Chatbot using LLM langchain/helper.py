# Conversationa Q&A Chatbot

# 1. Imports:
import os
import streamlit as st          
from langchain_community.chat_models import ChatOpenAI
from langchain.schema import HumanMessage,SystemMessage,AIMessage       ##  Imports classes for structuring messages in the conversation:

## HumanMessage = Represents messages sent by the user.
## SystemMessage = Represents instructions or context provided to the chatbot. e.g:-["you need to act like comedy chatbot"] that actually makes SystemMessage.
## AIMessage = Represents responses generated by the chatbot..
## So we need to store all this particular value in Session {HumanMessage, SystemMessage, AIMessage}. So that the entier chatbot will be able to remember the context.

from streamlit.commands import page_config
from dotenv import load_dotenv
load_dotenv()


# 2. Initializing the Chat Model
chat_llm = ChatOpenAI(temperature = 0.3)

# 3. Setting Up Session State
## By default is their is no session as such the first message that should probably going is this system message saying the chatbot act as a professional AI assistant

if 'flowmessage' not in st.session_state:
    st.session_state['flowmessage'] = [
        SystemMessage(content="you are a professional AI assistant")
    ]

    ## st.session_state: Stores information across user interactions within a Streamlit session, maintaining context for the chatbot conversation.
    ## helps the chatbot remember what you talked about before, so the conversation makes sense.
## This code snippet checks if the session state contains a key named 'flowmessage'. 
## If not, it initializes it with a default message indicating that the chatbot acts as a professional AI assistant.


# 4. Defining the Response Function
def get_chatmodel_response(question):                                               ## This function takes a user's question as input and returns the chatbot's response.
    #setup session_state here as "flowmessage"
    st.session_state['flowmessage'].append(HumanMessage(content=question))          ## The function appends the user's question as a HumanMessage to the flowmessage list in the session state
    answer = chat_llm(st.session_state['flowmessage'])                              ## The flowmessage list, containing the conversation history, is passed to the chat_llm model to generate a response.
    st.session_state['flowmessage'].append(AIMessage(content=answer.content))       ## The generated response is then appended to the flowmessage list as an AIMessage [The .content attribute is used to extract the text content from the answer object.]
    return answer.content                                                           ## Content of the chatbot's response is returned by the function