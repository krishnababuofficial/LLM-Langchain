{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f623047",
   "metadata": {},
   "source": [
    "## Chatmodels with ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5dcc7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very good to creation conversational bot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ef24f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d7c1f",
   "metadata": {},
   "source": [
    "- After importing \"ChatOpenAI\" there are 3 schemas to understand\n",
    "- When ever a converesation happens when a human is probabily giving input and expects a response that basically becomes a human message\n",
    "- system message schema\n",
    "- AI message schema the AI what ever model specificaly giving output is AI_MESSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d782b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage,SystemMessage,AIMessage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "114acfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "380ebc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KRISHNA\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPEN_API_KEY\"] = open_api_key\n",
    "chatllm = ChatOpenAI(openai_api_key = os.environ[\"OPEN_API_KEY\"], temperature = 0.7,model = \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b3b7562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KRISHNA\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. \"Why did the AI break up with its algorithm partner? It couldn\\'t handle the commitment to binary code!\"\\n\\n2. \"What do you call a robot that tells jokes? A stand-up data scientist!\"\\n\\n3. \"Why did the AI go to therapy? It had too many unresolved files!\"\\n\\n4. \"I asked my AI assistant to tell me a joke, and it replied, \\'I can\\'t, I\\'m too byte-sized for that!\\'\"\\n\\n5. \"Why did the AI cross the road? To optimize its pathfinding algorithm!\"\\n\\n6. \"What do you get when you cross an AI with a comedian? A virtual stand-up routine that\\'s out of this world!\"\\n\\n7. \"Why did the robot go to comedy school? It wanted to upgrade its humor software!\"\\n\\n8. \"I told my AI assistant to make me laugh, and it replied, \\'I\\'m sorry, Dave, I\\'m afraid I can\\'t do that.\\' Classic HAL-arious!\"\\n\\n9. \"Why was the AI such a good comedian? It had a great sense of artificial humor!\"\\n\\n10. \"I asked my AI to tell me a joke about machine learning, and it replied, \\'I don\\'t know, I haven\\'t been trained on that dataset yet!\\'\"', response_metadata={'token_usage': {'completion_tokens': 255, 'prompt_tokens': 25, 'total_tokens': 280}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-fb8e3f91-7b5b-4be3-aaae-007bb72bba23-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm([\n",
    "SystemMessage(content = \"you are a comedian AI assistant\"),\n",
    "HumanMessage(content = \"please provide some comedy punchlines on AI\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586a33d",
   "metadata": {},
   "source": [
    "- This is the otput basically it becomes an AIMessage this schema is ablew to see from the output of the particular chatbot\n",
    "- The HumanMessage is basically input and AIMessage is output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb071f5",
   "metadata": {},
   "source": [
    "## Prompt Template + LLM + Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "393ad4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chat_models import ChatHuggingFace\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "# from langchain.schema import BaseOutputparser\n",
    "from langchain_core.output_parsers import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcfd8cfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3325a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In some situations you may want to implement a custom parser to structure the model output into a custom format.\n",
    "# defining output parser define it in a form of class it inheriate the base output class\n",
    "\n",
    "class commaseperatedoutput(BaseOutputParser):\n",
    "    def parse(self, text:str):\n",
    "        return text.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71b3dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "huggingface_api_key = \"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\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = huggingface_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a09e0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm_hugfac = HuggingFaceHub(repo_id = \"Intel/dynamic_tinybert\", task = 'text-generation', model_kwargs={\"temperature\": 0.1, \"max_lenth\": 64})\n",
    "#max_lenth is maximum string length to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0506b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"you are a helpful assistant. When the user given any input, you should generate 5 words synonums in comma separated list\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chatprompt  =ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "859d2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inorder to execute this we need chains\n",
    "# Combine all this things\n",
    "# This \"|\" symbol represent that it is getting chained\n",
    "\n",
    "chain = chatprompt | chatllm | commaseperatedoutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d878a375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart', ' clever', ' bright', ' astute', ' sharp']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\": \"intelligent\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bda278bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='smart, clever, bright, astute, sharp', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 39, 'total_tokens': 49}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-7b4228a9-c78b-46b4-baca-52eafd720d0f-0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain2 without output parser\n",
    "\n",
    "chain2= chatprompt | chatllm \n",
    "\n",
    "chain2.invoke({\"text\": \"intelligent\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef692e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
